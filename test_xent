import file_utils as utils
from utilities import parse_arg
from numpy.random import seed
from datetime import datetime
from os import path
import os
import logging
from trainer.tf_generator import DataGenerator

logging.basicConfig(level=logging.DEBUG)

from tensorflow import set_random_seed
import tensorflow as tf

seed(1337)
set_random_seed(1337)

date_str = datetime.now().strftime("%Y-%m-%d_%H:%M:%S")
folder_str = 'model-inkml-' + date_str
weights_fname = 'weights_{epoch}.h5'
history_fname = 'history.pkl'
results_fname = 'results.pkl'

gcs = parse_arg('--gcs', required=False)
use_gpu = parse_arg('--gpu', default='n', required=False)
start_epoch = int(parse_arg('--start-epoch', 0))
data_base_dir = parse_arg('--data-base-dir', '/Users/balazs/new_data')
model_checkpoint_dir = parse_arg('--model-dir', '/Users/balazs/university/tf_model')
tensorboard_log_dir = parse_arg('--tb', None, required=False)
tensorboard_name = parse_arg('--tbn', "adam", required=False)
base_dir = path.join(model_checkpoint_dir, folder_str)
save_dir = base_dir
if gcs is not None:
    save_dir = os.path.join("gs://{}".format(gcs), save_dir)

#if not path.exists(base_dir):
#    os.mkdir(base_dir)

model_weights_file = path.join(model_checkpoint_dir, folder_str, weights_fname)
results_file = path.join(model_checkpoint_dir, folder_str, results_fname)
history_file = path.join(model_checkpoint_dir, folder_str, history_fname)

start_time = datetime.now()
git_hexsha = parse_arg('--git-hexsha', 'NAN')

batch_size = 16
epochs = 50
encoding_vb, decoding_vb = utils.read_pkl(path.join(data_base_dir, "vocabulary.pkl"))

image, truth, _ = zip(*utils.read_pkl(path.join(data_base_dir, "data_train.pkl")))
generator = DataGenerator(image, truth, encoding_vb, batch_size)


input_labels = tf.placeholder(dtype=tf.int32, shape=(batch_size, None))
input_lengths = tf.placeholder(dtype=tf.int32, shape=(batch_size,))

shape = tf.shape(input_labels)
constant_logits = tf.ones_like(input_labels, dtype=tf.float32)
constant_logits = tf.reshape(tf.tile(constant_logits, [1, len(encoding_vb)]), [batch_size, -1, len(encoding_vb)])

sequence_masks = tf.sequence_mask(input_lengths, dtype=tf.float32)
loss = tf.contrib.seq2seq.sequence_loss(constant_logits, input_labels, sequence_masks)

with tf.Session() as sess:
    for i in range(10):
        generator.reset()
        for i in range(generator.steps()):
            image, label, observation, masks, lengths = generator.next_batch()
            sess.run([loss], feed_dict={
                input_labels: [[a + 2 for a in l] for l in label ],
                input_lengths: lengths
            })
